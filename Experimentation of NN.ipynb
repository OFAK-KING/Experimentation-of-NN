{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from keras.models import Sequential as Seq\n",
    "from keras.layers import Dense\n",
    "from keras.constraints import unit_norm #to force weights to have a magnitude of 1.0\n",
    "from keras.optimizers import SGD \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing and Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1592, 266)\n",
      "1592\n"
     ]
    }
   ],
   "source": [
    "path = '../Codes/'\n",
    "filename = 'semeionCSV.csv'\n",
    "df = pd.read_csv(path+filename)\n",
    "print(df.shape) #checking the shape of the dataset\n",
    "print(len(df)) # checking if the full rows of the data was imported  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 266)\n",
    "#print(df.head()) #printing out few entries of the data from the top row eventhough have accessed it in notpad and excel++\n",
    "#print(df.tail()) #printing out few entries of the data from the bottom row eventhough have accessed it in notpad++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if there is column tht contains all zeros\n",
    "df = df.loc[:, (df != 0).any(axis=0)]\n",
    "#print(df.shape) #checking the shape of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividing the Dataset into Predictor and Response Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dependent = df[df.columns[256:]].copy()\n",
    "#print(Dependent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Independent = df[df.columns[:256]].copy()\n",
    "#print(Independent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Standardize** (to regress the data mean to 0 and standard deviation to 1) the features so that the model can converge fast "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CHERPENS\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:645: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\CHERPENS\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "standardize = preprocessing.StandardScaler().fit(Independent)\n",
    "Independent = standardize.transform(Independent)\n",
    "#print(Independent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1592, 256)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Independent.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating Training and Testing Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the Data into Training and Testing Set 70/30 respectively \n",
    "Indep_train, Indep_test, Dep_train, Dep_test = train_test_split(Independent, Dependent, test_size=0.30, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_Model_IM = Seq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding 50 Hidden and 10 output layers to the model\n",
    "NN_Model_IM.add(Dense(100, input_dim = 256, bias_constraint = unit_norm(axis = 0),\n",
    "                   activation = 'sigmoid'))\n",
    "NN_Model_IM.add(Dense(10, bias_constraint = unit_norm(axis = 0), activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "BPSGD_opt = SGD(lr = 1) #Set the learning rate to 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Compilation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_Model_IM.compile(optimizer = BPSGD_opt, loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 100)               25700     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 26,710\n",
      "Trainable params: 26,710\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "NN_Model_IM.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fitting the NN Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\CHERPENS\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 1114 samples, validate on 478 samples\n",
      "Epoch 1/50\n",
      "1114/1114 [==============================] - 0s 357us/step - loss: 1.1611 - acc: 0.6400 - val_loss: 0.7182 - val_acc: 0.7469\n",
      "Epoch 2/50\n",
      "1114/1114 [==============================] - 0s 53us/step - loss: 0.2635 - acc: 0.9291 - val_loss: 0.3730 - val_acc: 0.8745\n",
      "Epoch 3/50\n",
      "1114/1114 [==============================] - 0s 52us/step - loss: 0.1275 - acc: 0.9785 - val_loss: 0.2654 - val_acc: 0.9079\n",
      "Epoch 4/50\n",
      "1114/1114 [==============================] - 0s 56us/step - loss: 0.0742 - acc: 0.9910 - val_loss: 0.2779 - val_acc: 0.9121\n",
      "Epoch 5/50\n",
      "1114/1114 [==============================] - 0s 52us/step - loss: 0.0485 - acc: 0.9973 - val_loss: 0.2456 - val_acc: 0.9247\n",
      "Epoch 6/50\n",
      "1114/1114 [==============================] - 0s 53us/step - loss: 0.0328 - acc: 0.9991 - val_loss: 0.2381 - val_acc: 0.9247\n",
      "Epoch 7/50\n",
      "1114/1114 [==============================] - 0s 52us/step - loss: 0.0251 - acc: 1.0000 - val_loss: 0.2356 - val_acc: 0.9289\n",
      "Epoch 8/50\n",
      "1114/1114 [==============================] - 0s 55us/step - loss: 0.0206 - acc: 1.0000 - val_loss: 0.2325 - val_acc: 0.9268\n",
      "Epoch 9/50\n",
      "1114/1114 [==============================] - 0s 54us/step - loss: 0.0171 - acc: 1.0000 - val_loss: 0.2335 - val_acc: 0.9247\n",
      "Epoch 10/50\n",
      "1114/1114 [==============================] - 0s 62us/step - loss: 0.0148 - acc: 1.0000 - val_loss: 0.2362 - val_acc: 0.9247\n",
      "Epoch 11/50\n",
      "1114/1114 [==============================] - 0s 55us/step - loss: 0.0129 - acc: 1.0000 - val_loss: 0.2350 - val_acc: 0.9247\n",
      "Epoch 12/50\n",
      "1114/1114 [==============================] - 0s 63us/step - loss: 0.0115 - acc: 1.0000 - val_loss: 0.2356 - val_acc: 0.9268\n",
      "Epoch 13/50\n",
      "1114/1114 [==============================] - 0s 56us/step - loss: 0.0104 - acc: 1.0000 - val_loss: 0.2356 - val_acc: 0.9268\n",
      "Epoch 14/50\n",
      "1114/1114 [==============================] - 0s 52us/step - loss: 0.0094 - acc: 1.0000 - val_loss: 0.2345 - val_acc: 0.9268\n",
      "Epoch 15/50\n",
      "1114/1114 [==============================] - 0s 53us/step - loss: 0.0086 - acc: 1.0000 - val_loss: 0.2365 - val_acc: 0.9268\n",
      "Epoch 16/50\n",
      "1114/1114 [==============================] - 0s 57us/step - loss: 0.0079 - acc: 1.0000 - val_loss: 0.2376 - val_acc: 0.9247\n",
      "Epoch 17/50\n",
      "1114/1114 [==============================] - 0s 60us/step - loss: 0.0074 - acc: 1.0000 - val_loss: 0.2378 - val_acc: 0.9247\n",
      "Epoch 18/50\n",
      "1114/1114 [==============================] - 0s 58us/step - loss: 0.0068 - acc: 1.0000 - val_loss: 0.2381 - val_acc: 0.9247\n",
      "Epoch 19/50\n",
      "1114/1114 [==============================] - 0s 54us/step - loss: 0.0064 - acc: 1.0000 - val_loss: 0.2374 - val_acc: 0.9268\n",
      "Epoch 20/50\n",
      "1114/1114 [==============================] - 0s 60us/step - loss: 0.0060 - acc: 1.0000 - val_loss: 0.2382 - val_acc: 0.9268\n",
      "Epoch 21/50\n",
      "1114/1114 [==============================] - 0s 55us/step - loss: 0.0057 - acc: 1.0000 - val_loss: 0.2389 - val_acc: 0.9268\n",
      "Epoch 22/50\n",
      "1114/1114 [==============================] - 0s 59us/step - loss: 0.0054 - acc: 1.0000 - val_loss: 0.2397 - val_acc: 0.9268\n",
      "Epoch 23/50\n",
      "1114/1114 [==============================] - 0s 53us/step - loss: 0.0051 - acc: 1.0000 - val_loss: 0.2396 - val_acc: 0.9268\n",
      "Epoch 24/50\n",
      "1114/1114 [==============================] - 0s 53us/step - loss: 0.0048 - acc: 1.0000 - val_loss: 0.2400 - val_acc: 0.9268\n",
      "Epoch 25/50\n",
      "1114/1114 [==============================] - 0s 60us/step - loss: 0.0046 - acc: 1.0000 - val_loss: 0.2408 - val_acc: 0.9268\n",
      "Epoch 26/50\n",
      "1114/1114 [==============================] - 0s 61us/step - loss: 0.0044 - acc: 1.0000 - val_loss: 0.2414 - val_acc: 0.9268\n",
      "Epoch 27/50\n",
      "1114/1114 [==============================] - 0s 60us/step - loss: 0.0042 - acc: 1.0000 - val_loss: 0.2417 - val_acc: 0.9268\n",
      "Epoch 28/50\n",
      "1114/1114 [==============================] - 0s 61us/step - loss: 0.0040 - acc: 1.0000 - val_loss: 0.2424 - val_acc: 0.9268\n",
      "Epoch 29/50\n",
      "1114/1114 [==============================] - 0s 54us/step - loss: 0.0039 - acc: 1.0000 - val_loss: 0.2429 - val_acc: 0.9268\n",
      "Epoch 30/50\n",
      "1114/1114 [==============================] - 0s 53us/step - loss: 0.0037 - acc: 1.0000 - val_loss: 0.2433 - val_acc: 0.9268\n",
      "Epoch 31/50\n",
      "1114/1114 [==============================] - 0s 59us/step - loss: 0.0036 - acc: 1.0000 - val_loss: 0.2441 - val_acc: 0.9268\n",
      "Epoch 32/50\n",
      "1114/1114 [==============================] - 0s 61us/step - loss: 0.0034 - acc: 1.0000 - val_loss: 0.2447 - val_acc: 0.9268\n",
      "Epoch 33/50\n",
      "1114/1114 [==============================] - 0s 60us/step - loss: 0.0033 - acc: 1.0000 - val_loss: 0.2451 - val_acc: 0.9268\n",
      "Epoch 34/50\n",
      "1114/1114 [==============================] - 0s 54us/step - loss: 0.0032 - acc: 1.0000 - val_loss: 0.2454 - val_acc: 0.9268\n",
      "Epoch 35/50\n",
      "1114/1114 [==============================] - 0s 52us/step - loss: 0.0031 - acc: 1.0000 - val_loss: 0.2460 - val_acc: 0.9268\n",
      "Epoch 36/50\n",
      "1114/1114 [==============================] - 0s 59us/step - loss: 0.0030 - acc: 1.0000 - val_loss: 0.2461 - val_acc: 0.9268\n",
      "Epoch 37/50\n",
      "1114/1114 [==============================] - 0s 55us/step - loss: 0.0029 - acc: 1.0000 - val_loss: 0.2469 - val_acc: 0.9268\n",
      "Epoch 38/50\n",
      "1114/1114 [==============================] - 0s 53us/step - loss: 0.0028 - acc: 1.0000 - val_loss: 0.2470 - val_acc: 0.9268\n",
      "Epoch 39/50\n",
      "1114/1114 [==============================] - 0s 54us/step - loss: 0.0027 - acc: 1.0000 - val_loss: 0.2476 - val_acc: 0.9268\n",
      "Epoch 40/50\n",
      "1114/1114 [==============================] - 0s 79us/step - loss: 0.0026 - acc: 1.0000 - val_loss: 0.2484 - val_acc: 0.9268\n",
      "Epoch 41/50\n",
      "1114/1114 [==============================] - 0s 73us/step - loss: 0.0026 - acc: 1.0000 - val_loss: 0.2487 - val_acc: 0.9268\n",
      "Epoch 42/50\n",
      "1114/1114 [==============================] - 0s 74us/step - loss: 0.0025 - acc: 1.0000 - val_loss: 0.2487 - val_acc: 0.9268\n",
      "Epoch 43/50\n",
      "1114/1114 [==============================] - 0s 81us/step - loss: 0.0024 - acc: 1.0000 - val_loss: 0.2490 - val_acc: 0.9268\n",
      "Epoch 44/50\n",
      "1114/1114 [==============================] - 0s 78us/step - loss: 0.0024 - acc: 1.0000 - val_loss: 0.2493 - val_acc: 0.9268\n",
      "Epoch 45/50\n",
      "1114/1114 [==============================] - 0s 77us/step - loss: 0.0023 - acc: 1.0000 - val_loss: 0.2497 - val_acc: 0.9268\n",
      "Epoch 46/50\n",
      "1114/1114 [==============================] - 0s 74us/step - loss: 0.0023 - acc: 1.0000 - val_loss: 0.2501 - val_acc: 0.9268\n",
      "Epoch 47/50\n",
      "1114/1114 [==============================] - 0s 76us/step - loss: 0.0022 - acc: 1.0000 - val_loss: 0.2502 - val_acc: 0.9268\n",
      "Epoch 48/50\n",
      "1114/1114 [==============================] - 0s 76us/step - loss: 0.0021 - acc: 1.0000 - val_loss: 0.2507 - val_acc: 0.9268\n",
      "Epoch 49/50\n",
      "1114/1114 [==============================] - 0s 75us/step - loss: 0.0021 - acc: 1.0000 - val_loss: 0.2514 - val_acc: 0.9268\n",
      "Epoch 50/50\n",
      "1114/1114 [==============================] - 0s 71us/step - loss: 0.0020 - acc: 1.0000 - val_loss: 0.2517 - val_acc: 0.9268\n"
     ]
    }
   ],
   "source": [
    "NN_Model_Capture = NN_Model_IM.fit(Indep_train, Dep_train, epochs = 50, \n",
    "                                validation_data = (Indep_test, Dep_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy on Training AND Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Training Set: 1.0000\n",
      "'Loss for Training Set: 0.0020\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = NN_Model_IM.evaluate(Indep_train, Dep_train, verbose=0)\n",
    "print(\"Accuracy for Training Set: \"+\"{:.4f}\".format(accuracy))\n",
    "print(\"'Loss for Training Set: \"+\"{:.4f}\".format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Test Set: 0.9268\n",
      "Loss for Test Set: 0.2517\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = NN_Model_IM.evaluate(Indep_test, Dep_test, verbose=0)\n",
    "print(\"Accuracy for Test Set: \"+\"{:.4f}\".format(accuracy))\n",
    "print(\"Loss for Test Set: \"+\"{:.4f}\".format(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dependent_Pred = NN_Model_IM.predict(Indep_test)\n",
    "Predicted_classes = np.argmax(Dependent_Pred, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[49,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0, 45,  0,  0,  0,  0,  0,  1,  0,  1],\n",
       "       [ 1,  0, 48,  0,  0,  0,  0,  0,  1,  0],\n",
       "       [ 0,  0,  0, 36,  1,  2,  0,  0,  0,  2],\n",
       "       [ 0,  2,  0,  0, 48,  0,  1,  0,  0,  1],\n",
       "       [ 0,  0,  0,  2,  0, 49,  0,  0,  1,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0, 46,  0,  0,  0],\n",
       "       [ 1,  1,  0,  0,  2,  0,  0, 37,  0,  0],\n",
       "       [ 1,  1,  4,  0,  0,  0,  0,  1, 39,  1],\n",
       "       [ 1,  1,  0,  3,  0,  0,  0,  1,  1, 46]], dtype=int64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Confusion_Matrix = confusion_matrix(Dep_test.values.argmax(axis=1), Predicted_classes)\n",
    "Confusion_Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summarized Result on the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      1.00      0.96        49\n",
      "           1       0.90      0.96      0.93        47\n",
      "           2       0.92      0.96      0.94        50\n",
      "           3       0.88      0.88      0.88        41\n",
      "           4       0.94      0.92      0.93        52\n",
      "           5       0.96      0.94      0.95        52\n",
      "           6       0.98      1.00      0.99        46\n",
      "           7       0.93      0.90      0.91        41\n",
      "           8       0.93      0.83      0.88        47\n",
      "           9       0.90      0.87      0.88        53\n",
      "\n",
      "   micro avg       0.93      0.93      0.93       478\n",
      "   macro avg       0.93      0.93      0.93       478\n",
      "weighted avg       0.93      0.93      0.93       478\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Summarized_Result = classification_report(Dep_test.values.argmax(axis=1), Predicted_classes)\n",
    "print(Summarized_Result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Give a plot of both training and test accuracy as a function of epoch number (graph both of these on sample plot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "Capture_acc = NN_Model_Capture.history['acc']\n",
    "Capture_val_acc = NN_Model_Capture.history['val_acc']\n",
    "Capture_loss = NN_Model_Capture.history['loss']\n",
    "Capture_val_loss = NN_Model_Capture.history['val_loss']\n",
    "epochs = range(1, len(Capture_acc) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epochs, Capture_acc, 'bo', label = 'Training Accuracy')\n",
    "plt.plot(epochs, Capture_val_acc, 'b', label = 'Test Accuracy')\n",
    "plt.title('Training and Test Accuracy as a Function of Epoch Number')\n",
    "plt.legend()\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Just by Standardizing the dataset a model with the same parameters accuracies improved from;  0.9928 to 1 and losses regressed from 0.0973 to 0.0370 for the training set and the test set from 0.9121/ 0.2830 to 0.9310/0.2467 for accuracies and losses respectively. Hence I felt I did not need to  add  any more features and hyperparameters**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ============================================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardize = preprocessing.StandardScaler().fit(Independent)\n",
    "Independent = standardize.transform(Independent)\n",
    "#print(Independent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1592, 256)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Independent.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating Training and Testing Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the Data into Training and Testing Set 70/30 respectively \n",
    "Indep_train, Indep_test, Dep_train, Dep_test = train_test_split(Independent, Dependent, test_size=0.30, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_Model_IMLR = Seq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\CHERPENS\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "#Adding 50 Hidden and 10 output layers to the model\n",
    "NN_Model_IMLR.add(Dense(100, input_dim = 256, bias_constraint = unit_norm(axis = 0),\n",
    "                   activation = 'sigmoid'))\n",
    "NN_Model_IMLR.add(Dense(10, bias_constraint = unit_norm(axis = 0), activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "BPSGD_opt = SGD(lr = 1) #Set the learning rate to 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Compilation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_Model_IMLR.compile(optimizer = BPSGD_opt, loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 100)               25700     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 26,710\n",
      "Trainable params: 26,710\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "NN_Model_IMLR.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "#def my_learning_rate(epoch, lrate):\n",
    "    #return lrate\n",
    " \n",
    "#lrs = LearningRateScheduler(my_learning_rate)\n",
    "ES = EarlyStopping(monitor='val_loss', patience = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fitting the NN Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\CHERPENS\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 1114 samples, validate on 478 samples\n",
      "Epoch 1/60\n",
      "1114/1114 [==============================] - 0s 351us/step - loss: 1.2995 - acc: 0.5691 - val_loss: 0.5915 - val_acc: 0.8096\n",
      "Epoch 2/60\n",
      "1114/1114 [==============================] - 0s 70us/step - loss: 0.3326 - acc: 0.9013 - val_loss: 0.3183 - val_acc: 0.9059\n",
      "Epoch 3/60\n",
      "1114/1114 [==============================] - 0s 64us/step - loss: 0.1438 - acc: 0.9704 - val_loss: 0.2943 - val_acc: 0.9163\n",
      "Epoch 4/60\n",
      "1114/1114 [==============================] - 0s 61us/step - loss: 0.0873 - acc: 0.9883 - val_loss: 0.2630 - val_acc: 0.9226\n",
      "Epoch 5/60\n",
      "1114/1114 [==============================] - 0s 70us/step - loss: 0.0530 - acc: 0.9946 - val_loss: 0.2595 - val_acc: 0.9247\n",
      "Epoch 6/60\n",
      "1114/1114 [==============================] - 0s 65us/step - loss: 0.0368 - acc: 0.9973 - val_loss: 0.2516 - val_acc: 0.9268\n",
      "Epoch 7/60\n",
      "1114/1114 [==============================] - 0s 64us/step - loss: 0.0270 - acc: 0.9991 - val_loss: 0.2467 - val_acc: 0.9289\n",
      "Epoch 8/60\n",
      "1114/1114 [==============================] - 0s 73us/step - loss: 0.0215 - acc: 1.0000 - val_loss: 0.2505 - val_acc: 0.9226\n",
      "Epoch 9/60\n",
      "1114/1114 [==============================] - 0s 73us/step - loss: 0.0180 - acc: 1.0000 - val_loss: 0.2485 - val_acc: 0.9226\n",
      "Epoch 10/60\n",
      "1114/1114 [==============================] - 0s 71us/step - loss: 0.0152 - acc: 1.0000 - val_loss: 0.2476 - val_acc: 0.9184\n",
      "Epoch 11/60\n",
      "1114/1114 [==============================] - 0s 68us/step - loss: 0.0134 - acc: 1.0000 - val_loss: 0.2480 - val_acc: 0.9226\n",
      "Epoch 12/60\n",
      "1114/1114 [==============================] - 0s 81us/step - loss: 0.0119 - acc: 1.0000 - val_loss: 0.2512 - val_acc: 0.9205\n",
      "Epoch 13/60\n",
      "1114/1114 [==============================] - 0s 108us/step - loss: 0.0107 - acc: 1.0000 - val_loss: 0.2507 - val_acc: 0.9226\n",
      "Epoch 14/60\n",
      "1114/1114 [==============================] - 0s 92us/step - loss: 0.0097 - acc: 1.0000 - val_loss: 0.2487 - val_acc: 0.9226\n",
      "Epoch 15/60\n",
      "1114/1114 [==============================] - 0s 97us/step - loss: 0.0088 - acc: 1.0000 - val_loss: 0.2514 - val_acc: 0.9205\n",
      "Epoch 16/60\n",
      "1114/1114 [==============================] - 0s 131us/step - loss: 0.0082 - acc: 1.0000 - val_loss: 0.2527 - val_acc: 0.9184\n",
      "Epoch 17/60\n",
      "1114/1114 [==============================] - 0s 103us/step - loss: 0.0075 - acc: 1.0000 - val_loss: 0.2514 - val_acc: 0.9205\n"
     ]
    }
   ],
   "source": [
    "NN_Model_Capture = NN_Model_IMLR.fit(Indep_train, Dep_train, epochs = 60, \n",
    "                                validation_data = (Indep_test, Dep_test),\n",
    "                                    callbacks = [ES])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy on Training AND Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Training Set: 1.0000\n",
      "'Loss for Training Set: 0.0069\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = NN_Model_IMLR.evaluate(Indep_train, Dep_train, verbose=0)\n",
    "print(\"Accuracy for Training Set: \"+\"{:.4f}\".format(accuracy))\n",
    "print(\"'Loss for Training Set: \"+\"{:.4f}\".format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Test Set: 0.9205\n",
      "Loss for Test Set: 0.2514\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = NN_Model_IMLR.evaluate(Indep_test, Dep_test, verbose=0)\n",
    "print(\"Accuracy for Test Set: \"+\"{:.4f}\".format(accuracy))\n",
    "print(\"Loss for Test Set: \"+\"{:.4f}\".format(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Even though just by  Standardizing the dataset a model with the same parameters accuracies improved from;  0.9928 to 1 and losses regressed from 0.0973 to 0.0370 for the training set and the test set from 0.9121/ 0.2830 to 0.9310/0.2467 for accuracies and losses respectively. I decided to see if by experimenting with some features and hyperparameters if the gap between training and test could be smaller than what I got with my initial proved model but it was a fruitless exercise but loss for training set regressed to 0.0062, the only positive.  Examples of some of the hyperparameters there were introduced were left as comments in the code.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
